# 目的
- Chainerに関するありったけの知識を共有
  - 生兵法です　間違いがあったら指摘してください
  - for Chainerにあまり触った事がない人
- 先日バージョンアップされ、以前と異なる書き方ができるようになったので、その変更点を知ってもらう
  - for Chainerに最近触ってない人
- RNNを少し説明
  - for 興味ある方

# Chainerとは?
- Preferred Networksが開発したニューラルネットワークを実装するためのPythonライブラリ
  - 言語Python
  - Ubuntuが推奨されている
  - ライセンスはMIT（Expat） License
- Preferred Networks : IoTにフォーカスしたリアルタイム機械学習技術のビジネス活用を目的とした会社
  - 最近DeNAと合弁会社を設立したことで話題に　http://jp.techcrunch.com/2016/07/14/dena-pfn-ai/
- Chainerの特徴は下記のとおり（HPより抜粋）
  - 高速: CUDAをサポートし、GPUを利用した高速な計算が可能
  - 柔軟: 柔軟な記法により、畳み込み、リカレントなど、様々なタイプのニューラルネットを実装可能
  - 直観的: ネットワーク構成を直観的に記述できる
  - 多分tensorflowより同じことをやるにしてもコード量が少ない
  - TensorBoardみたいな可視化ツールはない
  - Caffeのmodel zooが使える
  - ver1.5以前と以後で書き方が全然違う
- インストールは 「pip install chainer」で行える
  - ただし、インストールが簡単というのは嘘（Windowsに導入する場合だけかもしれませんが・・・）
- CPUでは遅い（TensorFlowより遅い）が、GPUを利用すると早い
  - 厳密に言えばCPUで実行した場合、epochを重ねるごとに1epochあたりの所要時間がどんどん増加していく
  - http://nonbiri-tereka.hatenablog.com/entry/2015/12/16/003512
- 参考資料
  - http://www.slideshare.net/unnonouno/chainer-59664785
  - http://www.slideshare.net/ryokuta/chainer-59180103
  - P23の比較が分かりやすい

# Demo
## 1.Introduction
- Chaienrの概要を図示
  - http://atrg.jp/ja/index.php?plugin=attach&pcmd=open&file=20151225-ATTA-oono.pdf&refer=ATTA2015
  - P27～38
- 変数：Variable
  - 型はfloat32
  - np.zeros()のデフォルトはfloat64なので、そこに留意する（1hハマる）
- 層：Link
- 計算グラフ：Chian
- 最適化アルゴリズム：Optimizer
- 入出力はテンソルとして扱う
- Variableオブジェクト：計算グラフのデータノード、Numpy or Cupyの多次元配列を保持する。
- Functionオブジェクト：計算グラフの（パラメータを持たない）演算ノード、Variableを受け取り、Variableを出力する
- Linkオブジェクト：Optimizerの最適化の対象
- Cain：Linkをまとめる
- Optimizer：勾配を用いてパラメータ最適化する　SGD , MomentumSGD , AdaGrad , RMSprop , Adam など
  - Linkを引数に渡してupdateすればよい

## GitHub上のChainer/masterにおけるexample集
- https://github.com/pfnet/chainer/tree/master/examples
- imagenet
- mnist
- modelzoo
- ptb → RNNを使ってPen Tree Bankを学習する
- sentiment
- vae → Variational AutoEncorder
- word2vec : C-bowとSkip-gram　の実装 

## 2.Mnist Sample
### def __call__ 
伝播の処理　tensorflowでいうところの　tf.nn.sigmoid　？
ChainerではForward処理を記述しておけば、逆伝播は勝手に処理してくれる

### L.Classifier
分類器を作成する際の「損失関数」「精度評価」をするための
Classifierがデフォルトで入っている

### MNIST Dataset
- TensorFlowでTutorial等で使うデータとして提供されている
  - dataset of handwrittend digits
  - 60,000 examples for training
  - 10,000 examples for testing
  - 28✕ 28 サイズ
  - values from 0 to 1

## 3 Chaienr ver1.11からの新機能紹介
- DeepLraningの訓練プロセス
  1. 訓練データセットのイテレーション
  2. ミニバッチに対する前処理
  3. ニューラルネットワークのForward/backward計算
  4. パラメータの更新
  5. 評価データセットにおける現在のパラメータの評価
  6. 中間結果をログに残す訓練プロセスを抽象化するクラスが実装された 
- これらを抽象化するための機能が導入された

### Dataset
1. 訓練データセットのイテレーション
2. ミニバッチに対する前処理

### Training
　Upadter
3. ニューラルネットワークのForward/backward計算
4. パラメータの更新
　Extenion
5. 評価データセットにおける現在のパラメータの評価
6. 中間結果をログに残す

## 3 ChaienrでRNN LSTMで夏目漱石っぽい文章を生成(Deep Soseki)
- 夏目漱石作品を学習させ、夏目漱石っぽい文章を生成するモデルの構築
  - 吾輩は猫である、虞美人草、こころ、三四郎、それから、坊ちゃん、道草
  - 2MBの文章データ　　　

### 使用しているアルゴリズムの概要
- Recurent Neural Network言語モデル生成による、時系列データ生成
- http://www.slideshare.net/Gushi/chainer-with-natural-language-processing-hands-on-54003769(p27)
- http://qiita.com/S346/items/24e875e3c5ac58f55810
  - 学習文章内において「ある単語が入力された際の、次の単語の出現確率を予測するモデル」　
  - 固定長ベクトルから固定長ベクトルの出現確率をモデル化
  - 全体的に見ると静的なモデルにみれる  - 1.作成したモデルを言語モデルとして扱う、2.単語の特徴ベクトルを作成する　という目的で使われる（word2vecの前身）
  - 隠れ層をLSTMにしており、長期間重みを引き継げる　（BPTTによる勾配消失問題　重みが何度も掛けられるので指数的に増加 or 消失）
  - http://www.slideshare.net/unnonouno/ss-43844132 15P～19P
  - 最近はLSTMの代わりにGRUというものもある　が性能差はあまり変わらないとの噂？
  - よくわかるLSTM http://qiita.com/KojiOhki/items/89cd7b69a8a6239d67ca
  - RNN LSTM seq2seq 理論的なとこは待っていくとNLPの知識が必要になってきて深みにはまっていく
- ChainerのRNN関連サンプル古い問題　→　Chainerの書き方が違う、Pythonのバージョンが古く書き直す手間

### コード
元ネタ
http://orientalrobotics.blogspot.jp/2015/08/rnn-aka-deepdazai.html
https://github.com/longjie/chainer-char-rnn
CPUで回した場合、学習に5日以上かかっている...


# Discussion



